{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m ipykernel install --user --name=pyspark_env --display-name \"Python (pyspark_env)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .appName(\"mr_prac\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mr_prac</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x12f802f40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: ./title.crew.tsv.gz\n",
      "TSV already exists: ./title.crew.tsv\n",
      "File already exists: ./name.basics.tsv.gz\n",
      "TSV already exists: ./name.basics.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def download_and_unzip(url, output_dir=\".\"):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    local_gz_path = os.path.join(output_dir, filename)\n",
    "    local_tsv_path = local_gz_path.replace(\".gz\", \"\")\n",
    "\n",
    "    # Skip download if already exists\n",
    "    if not os.path.exists(local_gz_path):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(local_gz_path, 'wb') as f:\n",
    "            shutil.copyfileobj(response.raw, f)\n",
    "        print(f\"Downloaded: {local_gz_path}\")\n",
    "    else:\n",
    "        print(f\"File already exists: {local_gz_path}\")\n",
    "\n",
    "    # Unzip .gz to .tsv\n",
    "    if not os.path.exists(local_tsv_path):\n",
    "        print(f\"Unzipping {filename}...\")\n",
    "        with gzip.open(local_gz_path, 'rb') as f_in:\n",
    "            with open(local_tsv_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"Unzipped to: {local_tsv_path}\")\n",
    "    else:\n",
    "        print(f\"TSV already exists: {local_tsv_path}\")\n",
    "\n",
    "    return local_tsv_path\n",
    "\n",
    "# IMDb dataset URLs\n",
    "urls = [\n",
    "    \"https://datasets.imdbws.com/title.crew.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/name.basics.tsv.gz\"\n",
    "]\n",
    "\n",
    "# Download and unzip all\n",
    "for url in urls:\n",
    "    download_and_unzip(url)\n",
    "\n",
    "\n",
    "# Load IMDb crew data\n",
    "crew_df = spark.read.csv(\"title.crew.tsv\", sep=\"\\t\", header=True, inferSchema=True, nullValue=\"\\\\N\")\n",
    "\n",
    "# Load name.basics for mapping nconst -> primaryName\n",
    "names_df = spark.read.csv(\"name.basics.tsv\", sep=\"\\t\", header=True, inferSchema=True, nullValue=\"\\\\N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|primaryName          |count|\n",
      "+---------------------+-----+\n",
      "|Johnny Manahan       |13154|\n",
      "|Saibal Banerjee      |12613|\n",
      "|Nivedita Basu        |12368|\n",
      "|Bert De Leon         |10365|\n",
      "|Anil v Kumar         |8984 |\n",
      "|Santosh Bhatt        |8495 |\n",
      "|Danie Joubert        |8282 |\n",
      "|Conrado Lumabas      |8024 |\n",
      "|Duma Ndlovu          |7986 |\n",
      "|Silvia Abravanel     |7437 |\n",
      "|Malu London          |7434 |\n",
      "|Henrique Martins     |7173 |\n",
      "|Shashank Bali        |7125 |\n",
      "|Mário Márcio Bandarra|6974 |\n",
      "|Paul Alter           |6888 |\n",
      "|Walter Avancini      |6421 |\n",
      "|Bruno De Paola       |6360 |\n",
      "|Kevin McCarthy       |6264 |\n",
      "|S. Kumaran           |6263 |\n",
      "|Dilip Kumar          |6112 |\n",
      "+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean and extract directors (some titles have multiple directors)\n",
    "crew_directors = crew_df.select(\"tconst\", explode(split(col(\"directors\"), \",\")).alias(\"director_id\")).na.drop()\n",
    "\n",
    "# Count number of titles per director\n",
    "director_counts = crew_directors.groupBy(\"director_id\").count().alias(\"title_count\")\n",
    "\n",
    "# Join with names to get actual director names\n",
    "director_with_names = director_counts.join(\n",
    "    names_df.select(\"nconst\", \"primaryName\"),\n",
    "    director_counts[\"director_id\"] == names_df[\"nconst\"],\n",
    "    \"inner\"\n",
    ").select(\"primaryName\", \"count\").orderBy(col(\"count\").desc())\n",
    "\n",
    "# Show top 20 directors\n",
    "director_with_names.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 5, 4, 3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark_env)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
